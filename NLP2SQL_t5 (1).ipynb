{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlkERNVTVNEH",
        "outputId": "6494fb12-6c5b-4eff-f6aa-d3a837bb270e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: sdv in /usr/local/lib/python3.11/dist-packages (1.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: boto3<2.0.0,>=1.28 in /usr/local/lib/python3.11/dist-packages (from sdv) (1.37.13)\n",
            "Requirement already satisfied: botocore<2.0.0,>=1.31 in /usr/local/lib/python3.11/dist-packages (from sdv) (1.37.13)\n",
            "Requirement already satisfied: cloudpickle>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (3.1.1)\n",
            "Requirement already satisfied: graphviz>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.20.3)\n",
            "Requirement already satisfied: copulas>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.12.1)\n",
            "Requirement already satisfied: ctgan>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.11.0)\n",
            "Requirement already satisfied: deepecho>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.7.0)\n",
            "Requirement already satisfied: rdt>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (1.15.0)\n",
            "Requirement already satisfied: sdmetrics>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (0.19.0)\n",
            "Requirement already satisfied: platformdirs>=4.0 in /usr/local/lib/python3.11/dist-packages (from sdv) (4.3.6)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2.0.0,>=1.28->sdv) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3<2.0.0,>=1.28->sdv) (0.11.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.3.0)\n",
            "Requirement already satisfied: plotly>=5.10.0 in /usr/local/lib/python3.11/dist-packages (from copulas>=0.12.1->sdv) (5.24.1)\n",
            "Requirement already satisfied: scipy>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from copulas>=0.12.1->sdv) (1.14.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from rdt>=1.14.0->sdv) (1.6.1)\n",
            "Requirement already satisfied: Faker>=17 in /usr/local/lib/python3.11/dist-packages (from rdt>=1.14.0->sdv) (37.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.10.0->copulas>=0.12.1->sdv) (9.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.31->sdv) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->rdt>=1.14.0->sdv) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->rdt>=1.14.0->sdv) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch accelerate sdv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRzUU2pAVN7O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, Dataset\n",
        "from sdv.single_table import CTGANSynthesizer  # For synthetic data generation\n",
        "import pandas as pd\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc4onZVPVPzU",
        "outputId": "4f0c952b-6820-4556-d2c5-fa6b8d1a2f1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"column_names\": [\n",
            "      [\n",
            "        -1,\n",
            "        \"*\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"perpetrator id\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"people id\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"date\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"year\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"location\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"country\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"killed\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"injured\"\n",
            "      ],\n",
            "      [\n",
            "        1,\n",
            "        \"people id\"\n",
            "      ],\n",
            "      [\n",
            "        1,\n",
            "        \"name\"\n",
            "      ],\n",
            "      [\n",
            "        1,\n",
            "        \"height\"\n",
            "      ],\n",
            "      [\n",
            "        1,\n",
            "        \"weight\"\n",
            "      ],\n",
            "      [\n",
            "        1,\n",
            "        \"home town\"\n",
            "      ]\n",
            "    ],\n",
            "    \"column_names_original\": [\n",
            "      [\n",
            "        -1,\n",
            "        \"*\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"Perpetrator_ID\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"People_ID\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"Date\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"Year\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"Location\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"Country\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"Killed\"\n",
            "      ],\n",
            "      [\n",
            "        0,\n",
            "        \"Injured\"\n",
            "      ],\n",
            "      [\n",
            "        1,\n",
            "        \"People_ID\"\n",
            "      ],\n",
            "      [\n",
            "        1,\n",
            "        \"Name\"\n",
            "      ],\n",
            "      [\n",
            "        1,\n",
            "        \"Height\"\n",
            "      ],\n",
            "      [\n",
            "        1,\n",
            "        \"Weight\"\n",
            "      ],\n",
            "      [\n",
            "        1,\n",
            "        \"Home Town\"\n",
            "      ]\n",
            "    ],\n",
            "    \"column_types\": [\n",
            "      \"text\",\n",
            "      \"number\",\n",
            "      \"number\",\n",
            "      \"text\",\n",
            "      \"number\",\n",
            "      \"text\",\n",
            "      \"text\",\n",
            "      \"number\",\n",
            "      \"number\",\n",
            "      \"number\",\n",
            "      \"text\",\n",
            "      \"number\",\n",
            "      \"number\",\n",
            "      \"text\"\n",
            "    ],\n",
            "    \"db_id\": \"perpetrator\",\n",
            "    \"foreign_keys\": [\n",
            "      [\n",
            "        2,\n",
            "        9\n",
            "      ]\n",
            "    ],\n",
            "    \"primary_keys\": [\n",
            "      1,\n",
            "      9\n",
            "    ],\n",
            "    \"table_names\": [\n",
            "      \"perpetrator\",\n",
            "      \"people\"\n",
            "    ],\n",
            "    \"table_names_original\": [\n",
            "      \"perpetrator\",\n",
            "      \"people\"\n",
            "    ]\n",
            "  }\n",
            "]\n",
            "[\n",
            "  {\n",
            "    \"db_id\": \"department_management\",\n",
            "    \"query\": \"SELECT count(*) FROM head WHERE age  >  56\",\n",
            "    \"query_toks\": [\n",
            "      \"SELECT\",\n",
            "      \"count\",\n",
            "      \"(\",\n",
            "      \"*\",\n",
            "      \")\",\n",
            "      \"FROM\",\n",
            "      \"head\",\n",
            "      \"WHERE\",\n",
            "      \"age\",\n",
            "      \">\",\n",
            "      \"56\"\n",
            "    ],\n",
            "    \"query_toks_no_value\": [\n",
            "      \"select\",\n",
            "      \"count\",\n",
            "      \"(\",\n",
            "      \"*\",\n",
            "      \")\",\n",
            "      \"from\",\n",
            "      \"head\",\n",
            "      \"where\",\n",
            "      \"age\",\n",
            "      \">\",\n",
            "      \"value\"\n",
            "    ],\n",
            "    \"question\": \"How many heads of the departments are older than 56 ?\",\n",
            "    \"question_toks\": [\n",
            "      \"How\",\n",
            "      \"many\",\n",
            "      \"heads\",\n",
            "      \"of\",\n",
            "      \"the\",\n",
            "      \"departments\",\n",
            "      \"are\",\n",
            "      \"older\",\n",
            "      \"than\",\n",
            "      \"56\",\n",
            "      \"?\"\n",
            "    ],\n",
            "    \"sql\": {\n",
            "      \"from\": {\n",
            "        \"table_units\": [\n",
            "          [\n",
            "            \"table_unit\",\n",
            "            1\n",
            "          ]\n",
            "        ],\n",
            "        \"conds\": []\n",
            "      },\n",
            "      \"select\": [\n",
            "        false,\n",
            "        [\n",
            "          [\n",
            "            3,\n",
            "            [\n",
            "              0,\n",
            "              [\n",
            "                0,\n",
            "                0,\n",
            "                false\n",
            "              ],\n",
            "              null\n",
            "            ]\n",
            "          ]\n",
            "        ]\n",
            "      ],\n",
            "      \"where\": [\n",
            "        [\n",
            "          false,\n",
            "          3,\n",
            "          [\n",
            "            0,\n",
            "            [\n",
            "              0,\n",
            "              10,\n",
            "              false\n",
            "            ],\n",
            "            null\n",
            "          ],\n",
            "          56.0,\n",
            "          null\n",
            "        ]\n",
            "      ],\n",
            "      \"groupBy\": [],\n",
            "      \"having\": [],\n",
            "      \"orderBy\": [],\n",
            "      \"limit\": null,\n",
            "      \"intersect\": null,\n",
            "      \"union\": null,\n",
            "      \"except\": null\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Load Spider dataset (Update path if needed)\n",
        "with open(\"/content/spider_extracted/spider_data/tables.json\", \"r\") as f:\n",
        "    tables_data = json.load(f)\n",
        "\n",
        "with open(\"/content/spider_extracted/spider_data/train_spider.json\", \"r\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "# Check schema structure\n",
        "print(json.dumps(tables_data[:1], indent=2))\n",
        "print(json.dumps(train_data[:1], indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7aKfqJCVuqD",
        "outputId": "94a66b21-8ab0-44c4-85b2-941fe9a3b9a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema:\n",
            "  Object: perpetrator\n",
            "    Attributes: perpetrator id, people id, date, year, location, country, killed, injured\n",
            "  Object: people\n",
            "    Attributes: people id, name, height, weight, home town\n",
            "  Relationship: 1 → 0 (FK: name)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def convert_to_orass(schema):\n",
        "    tables = schema[\"table_names\"]\n",
        "    columns = schema[\"column_names\"][1:]  # Skip wildcard\n",
        "    relationships = schema.get(\"foreign_keys\", [])\n",
        "\n",
        "    formatted_schema = \"Schema:\\n\"\n",
        "    for table in tables:\n",
        "        formatted_schema += f\"  Object: {table}\\n\"\n",
        "        table_columns = [col[1] for col in columns if col[0] == tables.index(table)]\n",
        "        formatted_schema += f\"    Attributes: {', '.join(table_columns)}\\n\"\n",
        "\n",
        "    for fk in relationships:\n",
        "        parent_table, parent_col = columns[fk[0]]\n",
        "        child_table, child_col = columns[fk[1]]\n",
        "        formatted_schema += f\"  Relationship: {child_table} → {parent_table} (FK: {child_col})\\n\"\n",
        "\n",
        "    return formatted_schema\n",
        "\n",
        "# Convert a sample schema\n",
        "print(convert_to_orass(tables_data[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmKpOcPEVyYj",
        "outputId": "aeacef76-4729-4345-c87b-1b468f77d547"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORA-SS Schema Format:\n",
            " Schema:\n",
            "  Object: perpetrator\n",
            "    Attributes: perpetrator id, people id, date, year, location, country, killed, injured\n",
            "  Object: people\n",
            "    Attributes: people id, name, height, weight, home town\n",
            "  Relationship: 1 → 0 (FK: name)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def convert_to_orass(schema):\n",
        "    tables = schema[\"table_names\"]\n",
        "    columns = schema[\"column_names\"][1:]  # Skip wildcard\n",
        "    relationships = schema.get(\"foreign_keys\", [])\n",
        "\n",
        "    formatted_schema = \"Schema:\\n\"\n",
        "    for table in tables:\n",
        "        formatted_schema += f\"  Object: {table}\\n\"\n",
        "        table_columns = [col[1] for col in columns if col[0] == tables.index(table)]\n",
        "        formatted_schema += f\"    Attributes: {', '.join(table_columns)}\\n\"\n",
        "\n",
        "    for fk in relationships:\n",
        "        parent_table, parent_col = columns[fk[0]]\n",
        "        child_table, child_col = columns[fk[1]]\n",
        "        formatted_schema += f\"  Relationship: {child_table} → {parent_table} (FK: {child_col})\\n\"\n",
        "\n",
        "    return formatted_schema\n",
        "\n",
        "# Convert and print a sample schema in ORA-SS format\n",
        "sample_orass_schema = convert_to_orass(tables_data[0])\n",
        "print(\"ORA-SS Schema Format:\\n\", sample_orass_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aIgR_t3V-Id",
        "outputId": "494f79cf-7a17-4556-9ef8-6dac92f20e3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sdv/single_table/base.py:119: FutureWarning: The 'SingleTableMetadata' is deprecated. Please use the new 'Metadata' class for synthesizers.\n",
            "  warnings.warn(DEPRECATION_MSG, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sdv/single_table/base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synthetic Table Data:\n",
            "    perpetrator id  people id      date  year  location  country  killed  \\\n",
            "0              39         68   3005065    67        58       25      25   \n",
            "1              34         59    449512    30        89       74      28   \n",
            "2              37         66  16507995    41        10       23      33   \n",
            "3              36         76  16083568    80        33      438      47   \n",
            "4              34         51   3914937    80        76       96      42   \n",
            "5              71         23  10593927    66        33      909      83   \n",
            "6              70         78  14311281    40        65      202      80   \n",
            "7              56         38   5309139    90        28        9      72   \n",
            "8              40         81  15944639    27        95      674      43   \n",
            "9              36         26   8345955    51        77       34      76   \n",
            "\n",
            "   injured  \n",
            "0        3  \n",
            "1       25  \n",
            "2       16  \n",
            "3       52  \n",
            "4        9  \n",
            "5        9  \n",
            "6        6  \n",
            "7        5  \n",
            "8        4  \n",
            "9       38  \n"
          ]
        }
      ],
      "source": [
        "from sdv.metadata import SingleTableMetadata\n",
        "from sdv.single_table import CTGANSynthesizer\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Extract a sample table from Spider\n",
        "table_name = tables_data[0][\"table_names\"][0]  # First table\n",
        "columns = [col[1] for col in tables_data[0][\"column_names\"] if col[0] == 0]  # Columns of the first table\n",
        "\n",
        "# Create a sample dataset (Fake sample data)\n",
        "real_data = pd.DataFrame({\n",
        "    col: [random.randint(1, 100) for _ in range(10)] for col in columns  # Generate random values\n",
        "})\n",
        "\n",
        "# Define metadata for SDV\n",
        "metadata = SingleTableMetadata()\n",
        "metadata.detect_from_dataframe(real_data)\n",
        "\n",
        "# Initialize SDV synthesizer with metadata\n",
        "synthesizer = CTGANSynthesizer(metadata)\n",
        "\n",
        "# Train SDV model\n",
        "synthesizer.fit(real_data)\n",
        "\n",
        "# Generate synthetic data\n",
        "synthetic_data = synthesizer.sample(10)\n",
        "print(\"Synthetic Table Data:\\n\", synthetic_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvKYPpG6WEI5",
        "outputId": "2d9c6104-fa4f-4bca-fbc6-b20113332fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synthetic Text-to-SQL Pairs:\n",
            " [{'question': 'What is the value of country in perpetrator?', 'sql_query': 'SELECT country FROM perpetrator LIMIT 5'}, {'question': 'What is the value of injured in perpetrator?', 'sql_query': 'SELECT injured FROM perpetrator LIMIT 5'}, {'question': 'What is the value of country in perpetrator?', 'sql_query': 'SELECT country FROM perpetrator LIMIT 5'}, {'question': 'What is the value of country in perpetrator?', 'sql_query': 'SELECT country FROM perpetrator LIMIT 5'}, {'question': 'What is the value of location in perpetrator?', 'sql_query': 'SELECT location FROM perpetrator LIMIT 5'}]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def generate_text_sql_pairs(synthetic_data, schema):\n",
        "    text_sql_pairs = []\n",
        "\n",
        "    for _, row in synthetic_data.iterrows():\n",
        "        table_name = schema[\"table_names\"][0]\n",
        "        col_name = random.choice([col[1] for col in schema[\"column_names\"] if col[0] == 0])\n",
        "\n",
        "        question = f\"What is the value of {col_name} in {table_name}?\"\n",
        "        sql_query = f\"SELECT {col_name} FROM {table_name} LIMIT 5\"\n",
        "\n",
        "        text_sql_pairs.append({\"question\": question, \"sql_query\": sql_query})\n",
        "\n",
        "    return text_sql_pairs\n",
        "\n",
        "# Generate Text-to-SQL pairs\n",
        "synthetic_text_sql = generate_text_sql_pairs(synthetic_data, tables_data[0])\n",
        "print(\"Synthetic Text-to-SQL Pairs:\\n\", synthetic_text_sql[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8BEMZPeWgpZ",
        "outputId": "2f12a238-6b1e-441a-87cb-87ab1e870b4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Advanced Text-to-SQL Pairs:\n",
            " [{'question': 'What are the records where country is 25?', 'sql_query': \"SELECT * FROM perpetrator WHERE country = '25';\"}, {'question': 'Group records by perpetrator id and count occurrences.', 'sql_query': 'SELECT perpetrator id, COUNT(*) FROM perpetrator GROUP BY perpetrator id;'}, {'question': 'Join perpetrator with people and return all records.', 'sql_query': 'SELECT * FROM perpetrator INNER JOIN people ON perpetrator.id = people.id;'}, {'question': 'Join perpetrator with people and return all records.', 'sql_query': 'SELECT * FROM perpetrator INNER JOIN people ON perpetrator.id = people.id;'}, {'question': 'Group records by year and count occurrences.', 'sql_query': 'SELECT year, COUNT(*) FROM perpetrator GROUP BY year;'}]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def generate_advanced_sql(synthetic_data, schema):\n",
        "    \"\"\"Generate complex SQL queries with WHERE, JOIN, and GROUP BY.\"\"\"\n",
        "    text_sql_pairs = []\n",
        "\n",
        "    table_name = schema[\"table_names\"][0]  # Use the first table for now\n",
        "    columns = [col[1] for col in schema[\"column_names\"] if col[0] == 0]  # Columns from first table\n",
        "\n",
        "    for _, row in synthetic_data.iterrows():\n",
        "        col_name = random.choice(columns)  # Pick a random column\n",
        "        value = row[col_name]  # Get a random value from synthetic data\n",
        "\n",
        "        # Choose a random SQL pattern\n",
        "        query_type = random.choice([\"WHERE\", \"JOIN\", \"GROUP BY\", \"COUNT\", \"SUM\"])\n",
        "\n",
        "        if query_type == \"WHERE\":\n",
        "            question = f\"What are the records where {col_name} is {value}?\"\n",
        "            sql_query = f\"SELECT * FROM {table_name} WHERE {col_name} = '{value}';\"\n",
        "\n",
        "        elif query_type == \"JOIN\" and len(schema[\"table_names\"]) > 1:\n",
        "            # Create a JOIN query if multiple tables exist\n",
        "            second_table = schema[\"table_names\"][1]\n",
        "            question = f\"Join {table_name} with {second_table} and return all records.\"\n",
        "            sql_query = f\"SELECT * FROM {table_name} INNER JOIN {second_table} ON {table_name}.id = {second_table}.id;\"\n",
        "\n",
        "        elif query_type == \"GROUP BY\":\n",
        "            question = f\"Group records by {col_name} and count occurrences.\"\n",
        "            sql_query = f\"SELECT {col_name}, COUNT(*) FROM {table_name} GROUP BY {col_name};\"\n",
        "\n",
        "        elif query_type == \"COUNT\":\n",
        "            question = f\"How many records exist in {table_name}?\"\n",
        "            sql_query = f\"SELECT COUNT(*) FROM {table_name};\"\n",
        "\n",
        "        elif query_type == \"SUM\":\n",
        "            question = f\"What is the total sum of {col_name}?\"\n",
        "            sql_query = f\"SELECT SUM({col_name}) FROM {table_name};\"\n",
        "\n",
        "        text_sql_pairs.append({\"question\": question, \"sql_query\": sql_query})\n",
        "\n",
        "    return text_sql_pairs\n",
        "\n",
        "# Generate synthetic SQL queries\n",
        "advanced_text_sql = generate_advanced_sql(synthetic_data, tables_data[0])\n",
        "\n",
        "# Display samples\n",
        "print(\"Sample Advanced Text-to-SQL Pairs:\\n\", advanced_text_sql[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgmnsS-KW6hV",
        "outputId": "b588011e-7792-4b51-92e3-b8d56a961477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged dataset saved as 'merged_train_spider.json'\n"
          ]
        }
      ],
      "source": [
        "# Load Spider train data\n",
        "with open(\"/content/spider_extracted/spider_data/train_spider.json\", \"r\") as f:\n",
        "    train_spider_data = json.load(f)\n",
        "\n",
        "# Merge synthetic queries\n",
        "merged_train_data = train_spider_data + advanced_text_sql\n",
        "\n",
        "# Save merged dataset\n",
        "with open(\"merged_train_spider.json\", \"w\") as f:\n",
        "    json.dump(merged_train_data, f, indent=4)\n",
        "\n",
        "print(\"Merged dataset saved as 'merged_train_spider.json'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRlxQzfhXEZe",
        "outputId": "d4581efc-794d-460b-a3dd-ac621b625363"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer\n",
        "import json\n",
        "\n",
        "# Load merged dataset\n",
        "with open(\"/content/merged_train_spider.json\", \"r\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
        "\n",
        "# Convert to T5 format\n",
        "def preprocess_function(example):\n",
        "    \"\"\"\n",
        "    Preprocesses a single example from the dataset.\n",
        "\n",
        "    Handles cases where 'sql_query' might be missing. If missing,\n",
        "    it uses 'query' instead, which is likely the key used in\n",
        "    the original Spider dataset.\n",
        "    \"\"\"\n",
        "    inputs = f\"Schema: {example['question']}\"\n",
        "    # Use 'query' if 'sql_query' is not found\n",
        "    targets = example.get(\"sql_query\", example.get(\"query\", \"\"))\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing\n",
        "train_dataset = [preprocess_function(example) for example in train_data] # Change map to a list comprehension to process individual examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N-GEoj_ahYM",
        "outputId": "d6463c61-b890-4768-aca1-37f4d9284bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Mar 15 10:51:56 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P0             30W /   70W |     162MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Kill all previous processes using GPU\n",
        "!nvidia-smi\n",
        "!kill -9 $(nvidia-smi | awk '$5==\"python\"{print $3}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "dYD9SUvcXeHK",
        "outputId": "f1b0a24e-9448-49d6-b750-ca2c199b594d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msyedaumaizaunsa\u001b[0m (\u001b[33msyedaumaizaunsa-srm-institute-of-science-and-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_105201-e08d4ar8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/syedaumaizaunsa-srm-institute-of-science-and-technology/huggingface/runs/e08d4ar8' target=\"_blank\">./fine_tuned_t5_sql</a></strong> to <a href='https://wandb.ai/syedaumaizaunsa-srm-institute-of-science-and-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/syedaumaizaunsa-srm-institute-of-science-and-technology/huggingface' target=\"_blank\">https://wandb.ai/syedaumaizaunsa-srm-institute-of-science-and-technology/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/syedaumaizaunsa-srm-institute-of-science-and-technology/huggingface/runs/e08d4ar8' target=\"_blank\">https://wandb.ai/syedaumaizaunsa-srm-institute-of-science-and-technology/huggingface/runs/e08d4ar8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='657' max='657' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [657/657 2:31:29, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>inf</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=657, training_loss=nan, metrics={'train_runtime': 9105.4458, 'train_samples_per_second': 2.31, 'train_steps_per_second': 0.072, 'total_flos': 4.5388051513344e+16, 'train_loss': nan, 'epoch': 2.9905848787446505})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Load model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\").to(\"cuda\")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_t5_sql\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=32,\n",
        "    num_train_epochs=3,\n",
        "    save_steps=1000,\n",
        "    evaluation_strategy=\"no\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    optim=\"adafactor\",\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "HFOT0ljbBwoY",
        "outputId": "3247b237-f4e2-43d3-fbe0-1f2b16542249"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'query'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-4095d4937fad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Run Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-4095d4937fad>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataset, tokenizer)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Schema: {sample['question']}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrue_sql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Tokenize input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'query'"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"/content/fine_tuned_t5_sql/checkpoint-657\").to(\"cuda\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
        "\n",
        "# Load validation dataset\n",
        "with open(\"/content/validation_spider.json\", \"r\") as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "# Function to compute accuracy\n",
        "def evaluate_model(model, dataset, tokenizer):\n",
        "    exact_match, logical_form_match, execution_match = 0, 0, 0\n",
        "    total = len(dataset)\n",
        "\n",
        "    for sample in dataset:\n",
        "        question = f\"Schema: {sample['query']}\"\n",
        "        true_sql = sample[\"query\"]\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(question, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
        "\n",
        "        # Generate SQL query\n",
        "        output_ids = model.generate(**inputs)\n",
        "        pred_sql = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Exact Match (EM)\n",
        "        if pred_sql.strip().lower() == true_sql.strip().lower():\n",
        "            exact_match += 1\n",
        "\n",
        "        # Logical Form Accuracy (LF) - Ignores formatting differences\n",
        "        if set(pred_sql.lower().split()) == set(true_sql.lower().split()):\n",
        "            logical_form_match += 1\n",
        "\n",
        "        # Execution Accuracy (EX) - Requires actual database execution (Optional)\n",
        "        # execution_match += execute_and_compare(pred_sql, true_sql)\n",
        "\n",
        "    print(f\"✅ Exact Match Accuracy (EM): {exact_match / total:.2%}\")\n",
        "    print(f\"✅ Logical Form Accuracy (LF): {logical_form_match / total:.2%}\")\n",
        "    # print(f\"✅ Execution Accuracy (EX): {execution_match / total:.2%}\") # Uncomment if database is available\n",
        "\n",
        "# Run Evaluation\n",
        "evaluate_model(model, val_data, tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH00GZbPDRfZ",
        "outputId": "137623a2-ea5f-4531-e172-c09f12cde5a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Exact Match Accuracy (EM): 0.00%\n",
            "✅ Logical Form Accuracy (LF): 0.00%\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"/content/fine_tuned_t5_sql/checkpoint-657\").to(\"cuda\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
        "\n",
        "# Load validation dataset\n",
        "with open(\"/content/validation_spider.json\", \"r\") as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "# Function to compute accuracy\n",
        "def evaluate_model(model, dataset, tokenizer):\n",
        "    exact_match, logical_form_match, execution_match = 0, 0, 0\n",
        "    total = len(dataset)\n",
        "\n",
        "    for sample in dataset:\n",
        "        question = f\"Schema: {sample['query']}\"\n",
        "        # Use 'query' if 'sql_query' is not found\n",
        "        true_sql = sample.get(\"sql_query\", sample.get(\"query\", \"\"))\n",
        "\n",
        "        inputs = tokenizer(question, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
        "\n",
        "        output_ids = model.generate(**inputs)\n",
        "        pred_sql = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        if pred_sql.strip().lower() == true_sql.strip().lower():\n",
        "            exact_match += 1\n",
        "\n",
        "        if set(pred_sql.lower().split()) == set(true_sql.lower().split()):\n",
        "            logical_form_match += 1\n",
        "\n",
        "    print(f\"✅ Exact Match Accuracy (EM): {exact_match / total:.2%}\")\n",
        "    print(f\"✅ Logical Form Accuracy (LF): {logical_form_match / total:.2%}\")\n",
        "\n",
        "evaluate_model(model, val_data, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        },
        "id": "AeRG3wXlFe2f",
        "outputId": "955ea8c1-3053-48af-aaaa-a79bd9504d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Find the personal name, family name, and author ID of the course author that teaches the most courses.\n",
            "Expected SQL: SELECT T1.personal_name ,  T1.family_name ,  T2.author_id FROM Course_Authors_and_Tutors AS T1 JOIN Courses AS T2 ON T1.author_id  =  T2.author_id GROUP BY T2.author_id ORDER BY COUNT(*) DESC LIMIT 1\n",
            "Generated SQL: T2.author_id DESC LIMIT 1 SELECT T1.author_i\n",
            "Question: Show the country names and the corresponding number of players.\n",
            "Expected SQL: SELECT Country_name ,  COUNT(*) FROM country AS T1 JOIN match_season AS T2 ON T1.Country_id  =  T2.Country GROUP BY T1.Country_name\n",
            "Generated SQL: FROM country AS T1 JOIN match_season AS T2 ON T2.Country\n",
            "Question: What are the name, origin and owner of each program?\n",
            "Expected SQL: SELECT name ,  origin ,  OWNER FROM program\n",
            "Generated SQL: ,,,, ,, , ,\n",
            "Question: What are the all games score and location of the school called Clemson?\n",
            "Expected SQL: SELECT t2.All_Games ,  t1.location FROM university AS t1 JOIN basketball_match AS t2 ON t1.school_id  =  t2.school_id WHERE team_name  =  'Clemson'\n",
            "Generated SQL: FROM t1.All_Games , t2.location FROM university AS t\n",
            "Question: Show all distinct region names ordered by their labels.\n",
            "Expected SQL: SELECT DISTINCT region_name FROM region ORDER BY Label\n",
            "Generated SQL: FROM region_name FROM label FROM label FROM label FROM label FROM label FROM label FROM label\n",
            "Question: Which building has the largest number of company offices? Give me the building name.\n",
            "Expected SQL: SELECT T2.name FROM Office_locations AS T1 JOIN buildings AS T2 ON T1.building_id  =  T2.id JOIN Companies AS T3 ON T1.company_id  =  T3.id GROUP BY T1.building_id ORDER BY COUNT(*) DESC LIMIT 1\n",
            "Generated SQL: T1.building_id FROM T1.company_id FROM T1.company_i\n",
            "Question: What is the aircraft name for the flight with number 99\n",
            "Expected SQL: SELECT T2.name FROM Flight AS T1 JOIN Aircraft AS T2 ON T1.aid  =  T2.aid WHERE T1.flno  =  99\n",
            "Generated SQL: T1.name FROM Flight AS T1 JOIN Aircraft AS T2 ON T1.\n",
            "Question: What are the addresses of the course authors who teach either \"operating system\" or \"data structure\" course.\n",
            "Expected SQL: SELECT T1.address_line_1 FROM Course_Authors_and_Tutors AS T1 JOIN Courses AS T2 ON T1.author_id  =  T2.author_id WHERE T2.course_name  =  \"operating system\" OR T2.course_name  =  \"data structure\"\n",
            "Generated SQL: FROM Course_Authors_and_Tutors AS T1 JOIN Courses\n",
            "Question: What are the ids of all the employees who authorize document destruction?\n",
            "Expected SQL: SELECT DISTINCT Destruction_Authorised_by_Employee_ID FROM Documents_to_be_destroyed\n",
            "Generated SQL: FROM Documents_to_be_destroyed FROM Documents_to_be\n",
            "Question: Find the numbers of different majors and cities.\n",
            "Expected SQL: SELECT count(DISTINCT major) ,  count(DISTINCT city_code) FROM student\n",
            "Generated SQL: FROM student FROM student FROM student FROM student FROM student FROM student FROM student FROM student FROM student FROM\n",
            "Question: Find the name of instructors who didn't teach any courses?\n",
            "Expected SQL: SELECT name FROM instructor WHERE id NOT IN (SELECT id FROM teaches)\n",
            "Generated SQL: id FROM teaches SELECT name FROM instructor WHERE id\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-e7d17efa4b37>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Run debug evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mevaluate_model_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-e7d17efa4b37>\u001b[0m in \u001b[0;36mevaluate_model_debug\u001b[0;34m(model, dataset, tokenizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Generate SQL query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mpred_sql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2256\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3257\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1892\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 )\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     ):\n\u001b[0;32m--> 675\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    676\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     ):\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         attention_output = self.SelfAttention(\n\u001b[1;32m    594\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# convert into half-precision if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1913\u001b[0m     \u001b[0;31m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m     \u001b[0;31m# https://github.com/pytorch/pytorch/pull/115074\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def evaluate_model_debug(model, dataset, tokenizer):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total = len(dataset)\n",
        "    exact_match = 0\n",
        "\n",
        "    for sample in dataset:\n",
        "        question = f\"Schema: {sample['query']}\"\n",
        "        true_sql = sample[\"query\"]\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(question, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
        "\n",
        "        # Generate SQL query\n",
        "        output_ids = model.generate(**inputs)\n",
        "        pred_sql = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Print sample comparisons\n",
        "        print(\"Question:\", sample[\"question\"])\n",
        "        print(\"Expected SQL:\", true_sql)\n",
        "        print(\"Generated SQL:\", pred_sql)\n",
        "\n",
        "        # Check exact match\n",
        "        if pred_sql.strip().lower() == true_sql.strip().lower():\n",
        "            exact_match += 1\n",
        "\n",
        "    print(f\"\\n✅ Exact Match Accuracy: {exact_match / total:.2%}\")\n",
        "\n",
        "# Run debug evaluation\n",
        "evaluate_model_debug(model, val_data, tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FovlZObtH1X_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
